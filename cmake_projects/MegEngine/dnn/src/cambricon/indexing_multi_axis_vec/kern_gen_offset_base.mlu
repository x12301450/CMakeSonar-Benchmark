#include "./kern.mlu.h"
#include "megdnn/internal/defs.h"
#include "mlu.h"

using namespace megdnn;
using namespace cambricon;
using namespace indexing_multi_axis_vec;

namespace {
template <int nidx, int idx_ndim>
// generate offset of the axes indexed
__mlu_entry__ void kgen_offset_base(GenOffsetBaseParam<nidx, idx_ndim> param) {
    auto oidx = taskId;
    while (oidx < param.size) {
        int offset = 0;
#pragma unroll
        for (int i = 0; i < nidx; ++i) {
            auto& indexer = param.indexer[i];
            int idx_flat = 0, coidx = oidx;
#pragma unroll
            // get the flatten index
            for (int j = idx_ndim - 1; j >= 0; --j) {
                int ax_idx;
                if (j) {
                    int next_coidx = coidx / indexer.shape[j - 1];
                    ax_idx = coidx - (next_coidx * indexer.shape[j - 1]);
                    coidx = next_coidx;
                } else {
                    ax_idx = coidx;
                }
                idx_flat += indexer.stride[j] * ax_idx;
            }
            // get the data index with flatten index
            int data_idx = indexer.ptr[idx_flat];
            data_idx += (data_idx < 0 ? param.data_shape[i] : 0);
            if (static_cast<uint32_t>(data_idx) >= param.data_shape[i]) {
                // cast to uint32 to handle both negative and overflow
                // TODO: add error tracker for cambricon.
                data_idx = 0;
            }
            // calculate offset from current index
            offset += data_idx * param.data_stride[i];
        }
        // sum offsets and store at offset table
        param.output[oidx] = offset;
        oidx += taskDim;
    }
}
}  // namespace

template <int nidx, int idx_ndim>
void indexing_multi_axis_vec::gen_offset_base(
        const GenOffsetBaseParam<nidx, idx_ndim>& param, cnrtQueue_t queue) {
    void (*kptr)(GenOffsetBaseParam<nidx, idx_ndim>) = kgen_offset_base<nidx, idx_ndim>;
    cnrtDim3_t dim{
            static_cast<unsigned int>(param.cluster_count) *
                    static_cast<unsigned int>(param.core_per_cluster),
            1, 1};
    cnrtFunctionType_t function_type = CNRT_FUNC_TYPE_BLOCK;
    (*kptr)<<<dim, function_type, queue>>>(param);
}

namespace megdnn {
namespace cambricon {
namespace indexing_multi_axis_vec {

#define INST(_m, _n) \
    template void gen_offset_base(const GenOffsetBaseParam<_m, _n>&, cnrtQueue_t);

MEGDNN_FOREACH_TENSOR_NDIM(INST, 1)
MEGDNN_FOREACH_TENSOR_NDIM(INST, 2)
MEGDNN_FOREACH_TENSOR_NDIM(INST, 3)
MEGDNN_FOREACH_TENSOR_NDIM(INST, 4)
MEGDNN_FOREACH_TENSOR_NDIM(INST, 5)
MEGDNN_FOREACH_TENSOR_NDIM(INST, 6)
MEGDNN_FOREACH_TENSOR_NDIM(INST, 7)

#undef INST

}  // namespace indexing_multi_axis_vec
}  // namespace cambricon
}  // namespace megdnn
